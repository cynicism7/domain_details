literature_dirs:
  - "./papers"
extensions:
  - ".pdf"
llm:
  provider: "openai_api"
  model: "Qwen2.5-VL-7B-Instruct"
  api_base: "http://127.0.0.1:1234/v1"
  api_key: "lm-studio"
  # 思考型模型易把 token 耗在 <think> 里导致截断：可增大 max_tokens（如 1024）或换用非思考型小模型（如 Qwen2.5-7B）提速且更稳
  max_tokens: 512
  temperature: 0.0
  # 可选：自定义 system_prompt，不填则用内置提示（约束模型不要 <think>、直接输出 JSON）
  # system_prompt: "你是分类器。禁止 <think>，只输出一行 {\"field\": \"学科名称\"}。"
# 可选：优先让 AI 从以下领域中选择，不贴近再自拟。不填则使用程序内置列表
# preferred_domains:
#   - 细胞生物学
#   - 分子生物学
#   - 免疫学
#   - 肿瘤学
#   - 公共卫生
#   - ...
# 送交的文献内容上限（仅标题/作者/研究团队/摘要），越小越快
max_chars_for_llm: 800
# 单次请求「系统提示+用户提示」总字符数上限，避免本地模型上下文过长导致内存越界（大批量文献时尤为重要）
max_prompt_chars: 4096
# 每处理 N 篇文献后调用 LM Studio 卸载模型再加载，清空显存/上下文，避免大批量时内存越界。不设或 0 表示不定期清理
clear_context_every_n: 50
output:
  db_path: "./literature_domains.db"
  export_csv: True
  csv_path: "./literature_domains.csv"
